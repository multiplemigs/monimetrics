input {
  file {
    path => "/var/log/httpd/access_log"
    type => "apache"
  }
  file {
    path => "/var/log/httpd/error_log"
    type => "apache"
  }
}

filter {
  if [type] == "apache" {
    grok {
      match => { "message" => "%{COMBINEDAPACHELOG}" }
    }
    geoip {
      source => "clientip"
      target => "geoip"
      database => "/etc/logstash/GeoLiteCity.dat"
      add_field => [ "[geoip][coordinates]", "%{[geoip][longitude]}" ]
      add_field => [ "[geoip][coordinates]", "%{[geoip][latitude]}"  ]
    }
    mutate {
      convert => [ "[geoip][coordinates]", "float"]
      convert => [ "bytes", "integer" ]
    }
    date {
      # Try to pull the timestamp from the 'timestamp' field (parsed above with
      # grok). The apache time format looks like: "18/Aug/2011:05:44:34 -0700"
      match => [ "timestamp", "dd/MMM/yyyy:HH:mm:ss Z" ]
    }
  }
  if [type] == "apache_error" {
    grok {
      patterns_dir => [ "/etc/logstash/patterns.d" ]
      match => { "message" => "%{APACHE_ERROR_LOG}" }
    }
    geoip {
      source => "clientip"
      target => "geoip"
      database => "/etc/logstash/GeoLiteCity.dat"
      add_field => [ "[geoip][coordinates]", "%{[geoip][longitude]}" ]
      add_field => [ "[geoip][coordinates]", "%{[geoip][latitude]}"  ]
    }
    mutate {
      convert => [ "[geoip][coordinates]", "float"]
    }
    date {
      # Try to pull the timestamp from the 'timestamp' field (parsed above with
      # grok). The apache time format looks like: "18/Aug/2011:05:44:34 -0700"
      match => [ "timestamp", "dd/MMM/yyyy:HH:mm:ss Z" ]
    }
  }
}

output {
  elasticsearch {
    host => "{{ ElasticSearch Endpoint }}"
    protocol => "http"
  }
  s3 {
    access_key_id => "{{ AWS KEY }}"
    secret_access_key => "{{ AWS SECRET }}"
    bucket => "logstash-apache"
    region => "us-east-1"
    #optionally set temp dir for s3
    #temporary_directory => "/var/logstash-apache"
    time_file => 60
  }
  stdout { codec => rubydebug }
}
